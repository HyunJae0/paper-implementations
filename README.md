# BERT-pytorch-implementation
This implementation simply follows the content of the paper, so the details are not perfect.

Hyperparameters related to computing resources, such as model size and batch size, were set to the minimum values that could be run on my computer.

I used data from Hugging Faceâ€™s datasets library for both pretraining and downstream fine-tuning.
